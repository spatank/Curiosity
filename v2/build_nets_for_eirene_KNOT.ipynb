{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "build_nets_for_eirene_KNOT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOegYxGvehi6uw8EDbLliv0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/spatank/Curiosity/blob/master/v2/build_nets_for_eirene_KNOT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWiXh1ZmCmP2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "os.chdir('/content/drive/My Drive/Curiosity_IGT/KNOT')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sn4osBmdC36M"
      },
      "source": [
        "!ls # run !ls to verify location "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4igZFZDC5vy"
      },
      "source": [
        "import pandas as pd\n",
        "!pip install wikipedia2vec\n",
        "from wikipedia2vec import Wikipedia2Vec\n",
        "import numpy as np\n",
        "from scipy.spatial.distance import cosine\n",
        "from scipy.io import savemat\n",
        "import networkx as nx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPJv08yxDGah"
      },
      "source": [
        "wiki_df = pd.read_csv('KNOT_data_raw.csv')\n",
        "wiki_df.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LslGplMeDoaF"
      },
      "source": [
        "# Assign UIDs to Wikipedia Pages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6Ibnf24DIdq"
      },
      "source": [
        "def clean_entity_name(name):\n",
        "  name = name.replace('/wiki/', '')\n",
        "  name = name.replace('_', ' ')\n",
        "  return name"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FstmpDzODYUf"
      },
      "source": [
        "First, we create unique identifiers (UIDs) for each page so that they can be used as nodes in a network representation. Then we clean the strings associated with each page by stripping redundant information such as `wiki/` and `_`. The UIDs and clean names are appended to the data frame as new columns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ndzzIRIDKyv"
      },
      "source": [
        "# create UID for each page\n",
        "source_nodes = set(wiki_df['SourceName'].tolist())\n",
        "target_nodes = set(wiki_df['TargetName'].tolist())\n",
        "source_nodes.update(target_nodes)\n",
        "node_set = {entity: name for name, entity in enumerate(source_nodes)}\n",
        "wiki_df['SourceUID'] = wiki_df['SourceName'].apply(lambda x: node_set[x])\n",
        "wiki_df['SrcNameClean'] = wiki_df['SourceName'].apply(lambda x: clean_entity_name(x))\n",
        "wiki_df['TargetUID'] = wiki_df['TargetName'].apply(lambda x: node_set[x])\n",
        "wiki_df['TgtNameClean'] = wiki_df['TargetName'].apply(lambda x: clean_entity_name(x))\n",
        "wiki_df.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aU9rSRIDced"
      },
      "source": [
        "# Measures of Trait Curiosity\n",
        "\n",
        "Extract all curiosity measures from the dataframe for each participant."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pA-RuUvDNa9"
      },
      "source": [
        "# joyous exploration\n",
        "# deprivation sensitivity\n",
        "# stress tolerance\n",
        "# social curiosity\n",
        "# thrill seeking\n",
        "five_D = wiki_df.groupby('ID', as_index = False)[['JE_5D', 'DS_5D', 'ST_5D', 'SC_5D', 'TS_5D']].mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jG_oLnhDv20"
      },
      "source": [
        "five_D.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crB2HO1aDP6a"
      },
      "source": [
        "# filename = 'five_D.mat'\n",
        "# mdic = {name: col.values for name, col in five_D.items()}\n",
        "# savemat(filename, mdic)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1xwBtqJDi4a"
      },
      "source": [
        "# Create Individual Networks\n",
        "\n",
        "Next, we split the data set by individual, and use the `SourceUID`, `TargetUID`, and `SemanticDist` columns to generate network representations of participants' Wikipedia exploration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLJAtF31Dgkj"
      },
      "source": [
        "# split the data by individual\n",
        "ID_groups = wiki_df.groupby('ID')\n",
        "for ID, group in ID_groups:\n",
        "  # enforce time ordering\n",
        "  group.sort_values(by = ['TimeOrder'], inplace = True)\n",
        "  network_df = group[['TimeOrder', 'SourceUID', 'SrcNameClean', 'TargetUID', 'TgtNameClean', 'SemanticDist']].reset_index(drop = True)\n",
        "  # create an empty network\n",
        "  G = nx.Graph()\n",
        "  all_adj = []\n",
        "  edge_info = []\n",
        "  # incrementally add nodes and edges to the network\n",
        "  for index, row in network_df.iterrows():\n",
        "    from_node = row.get('SrcNameClean')\n",
        "    to_node = row.get('TgtNameClean')\n",
        "    edge_weight = row.get('SemanticDist')\n",
        "    edge_info_dict = {'from': from_node, 'to': to_node, 'weight': edge_weight}\n",
        "    edge_info.append(edge_info_dict)\n",
        "    # add edge to the network\n",
        "    G.add_edge(from_node, to_node, weight = edge_weight)\n",
        "    adj_G = nx.linalg.graphmatrix.adjacency_matrix(G, weight = 'weight')\n",
        "    all_adj.append(adj_G)\n",
        "  # save subject data to .mat file\n",
        "  filename = 'subj_' + str(ID) + '.mat'\n",
        "  mdic = {'subj': ID, 'all_adj': all_adj, 'edge_info': edge_info}\n",
        "  savemat(filename, mdic)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}