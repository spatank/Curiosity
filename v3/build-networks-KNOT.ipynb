{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import bz2\n",
    "import xml.etree.ElementTree as ET\n",
    "import mwparserfromhell as mph\n",
    "\n",
    "class Dump:\n",
    "    \"\"\"``Dump`` loads and parses dumps from wikipedia from\n",
    "    ``path_xml`` with index ``path_idx``.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    idx: dictionary\n",
    "        ``{'page_name': (byte offset, page id, block size)}``\n",
    "        Cached. Lazy.\n",
    "    links: list of strings\n",
    "        All links.\n",
    "    article_links: list of strings\n",
    "        Article links (not files, categories, etc.)\n",
    "    years: list of int\n",
    "        Years in the History section of a wikipedia page\n",
    "        BC denoted as negative values\n",
    "    page: mwparserfromhell.wikicode\n",
    "        Current loaded wiki page\n",
    "    path_xml: string\n",
    "        Path to the zipped XML dump file.\n",
    "    path_idx: string\n",
    "        Path to the zipped index file.\n",
    "    offset_max: int\n",
    "        Maximum offset. Set as the size of the zipped dump.\n",
    "    cache: xml.etree.ElementTree.Node\n",
    "        Cache of the XML tree in current block\n",
    "    \"\"\"\n",
    "    MAX_YEAR = 2020\n",
    "    \n",
    "    def __init__(self, path_xml, path_idx):\n",
    "        self._idx = {}\n",
    "        self._links = []\n",
    "        self._article_links = []\n",
    "        self._years = []\n",
    "        self._page = None\n",
    "        self.path_xml = path_xml\n",
    "        self.path_idx = path_idx\n",
    "        self.offset_max = 0\n",
    "        self.cache = (0, None) # offset, cache\n",
    "        \n",
    "    @property\n",
    "    def idx(self):\n",
    "        if self._idx:\n",
    "            return self._idx\n",
    "        else:\n",
    "            print('Dump: Loading index...')\n",
    "            with bz2.BZ2File(self.path_idx, 'rb') as file:\n",
    "                lines = [line for line in file]\n",
    "            block_end = os.path.getsize(self.path_xml)\n",
    "            offset_prev = block_end\n",
    "            for line in reversed(lines):\n",
    "                offset, pid, name = line.strip().split(b':', 2)\n",
    "                offset, pid, name = (int(offset), int(pid), name.decode('utf8'))\n",
    "                block_end = offset_prev if offset < offset_prev else block_end\n",
    "                self._idx[name] = (offset, pid, block_end-offset)\n",
    "                offset_prev = offset\n",
    "            self.offset_max = max([x[0] for x in self._idx.values()])\n",
    "            print('Dump: Loaded.')\n",
    "            return self._idx\n",
    "    \n",
    "    @property\n",
    "    def links(self):\n",
    "        if self._links:\n",
    "            return self._links\n",
    "        elif self.page:\n",
    "            self._links = [str(x.title) for x in self.page.filter_wikilinks()]\n",
    "            self._links = [link.split('#')[0] for link in self._links]\n",
    "            self._links = [link.split(' ') for link in self._links]\n",
    "            self._links = [[words[0].capitalize()] + words[1:] for words in self._links]\n",
    "            self._links = [' '.join(words) for words in self._links]\n",
    "            return self._links\n",
    "        else:\n",
    "            return self._links\n",
    "    \n",
    "    @property\n",
    "    def article_links(self):\n",
    "        if self._article_links:\n",
    "            return self._article_links\n",
    "        elif self.links:\n",
    "            self._article_links = [x for x in self.links if ':' not in x]\n",
    "            return self._article_links\n",
    "        else:\n",
    "            return self._article_links\n",
    "    \n",
    "    @property\n",
    "    def years(self):\n",
    "        if self._years:\n",
    "            return self._years\n",
    "        elif self.page:\n",
    "            history = Dump.get_history(self.page)\n",
    "            top = self.page.get_sections()[0].strip_code()\n",
    "            self._years = Dump.filter_years(top + history)\n",
    "            return self._years\n",
    "        else:\n",
    "            return self._years\n",
    "    \n",
    "    @property\n",
    "    def page(self):\n",
    "        return self._page\n",
    "    \n",
    "    @page.setter\n",
    "    def page(self, page):\n",
    "        self._page = page\n",
    "        self._links = []\n",
    "        self._article_links = []\n",
    "        self._years = []\n",
    "    \n",
    "    def load_page(self, page_name, filter_top=False):\n",
    "        \"\"\"Loads & returs page (``mwparserfromhell.wikicode``)\n",
    "        named ``page_name`` from dump file. Returns only the\n",
    "        top section if ``filter_top``.\n",
    "        \"\"\"\n",
    "        if page_name not in self.idx.keys():\n",
    "            self.page = None\n",
    "            return\n",
    "        offset, pid, block_size = self.idx[page_name]\n",
    "        if offset == self.cache[0]:\n",
    "            root = self.cache[1]\n",
    "        else:\n",
    "            xml = Dump.fetch_block(self.path_xml, offset, block_size)\n",
    "            xml = b'<mediawiki>' + xml + b'</mediawiki>'*(offset != self.offset_max)\n",
    "            root = ET.fromstring(xml)\n",
    "            self.cache = (offset, root)\n",
    "        text = Dump.search_id(root, pid)\n",
    "        text = Dump.filter_top_section(text) if filter_top else text\n",
    "        self.page = mph.parse(text, skip_style_tags = True)\n",
    "        if self.page and 'REDIRECT' in self.page.strip_code():\n",
    "            redirect = self.page.filter_wikilinks()[0].title\n",
    "            return self.load_page(str(redirect))\n",
    "        else:\n",
    "            return self.page\n",
    "    \n",
    "    @staticmethod\n",
    "    def fetch_block(path, offset, block_size):\n",
    "        \"\"\" Fetches block of ``block_size`` (``int``) bytes\n",
    "        at ``offset`` (``int``) in the zipped dump at \n",
    "        ``path`` (``string``) and returns the uncompressed\n",
    "        text (``string``).\n",
    "        \"\"\"\n",
    "        with open(path, 'rb') as file:\n",
    "            file.seek(offset)\n",
    "            return bz2.decompress(file.read(block_size))\n",
    "    \n",
    "    @staticmethod\n",
    "    def search_id(root, pid):\n",
    "        \"\"\"Returns the text of the page with id ``pid``\"\"\"\n",
    "        for page in root.iter('page'):\n",
    "            if pid == int(page.find('id').text):\n",
    "                return page.find('revision').find('text').text\n",
    "    \n",
    "    @staticmethod\n",
    "    def filter_top_section(text):\n",
    "        \"\"\"Returns the top section of text,\n",
    "        where the first header has the form ``==Heading==``\n",
    "        \"\"\"\n",
    "        head = re.search(r'==.*?==', text)\n",
    "        idx = head.span(0)[0] if head else len(text)\n",
    "        return text[:idx] #(text[:idx], text[idx:])\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_history(page):\n",
    "        \"\"\"Returns the text of the history section.\n",
    "        Returns ``\"\"`` if not found.\n",
    "        \"\"\"\n",
    "        headings = page.filter_headings()\n",
    "        idx = [i for i, head in enumerate(headings) \n",
    "                       if 'History' in head or 'history' in head]\n",
    "        if not idx:\n",
    "            return \"\"\n",
    "        sections = page.get_sections(include_headings=True)\n",
    "        history = str(sections[idx[0]+1].strip_code())\n",
    "        return history\n",
    "    \n",
    "    @staticmethod\n",
    "    def filter_years(text):\n",
    "        \"\"\"Filters the years from text.\"\"\"\n",
    "        months = ['january', 'february', 'march', 'april', 'may', 'june',\n",
    "                  'july', 'august', 'september', 'october', 'november', 'december']\n",
    "        prepositions = ['around', 'after', 'at', 'as',\n",
    "                        'approximately', 'before', 'between', 'by',\n",
    "                        'during', 'from', 'in', 'near', 'past',\n",
    "                        'since', 'until', 'within'] # removed: about, on\n",
    "        conjugations = ['and']\n",
    "        articles = ['the']\n",
    "        times = ['early', 'mid', 'late']\n",
    "        patterns = months + prepositions + conjugations + articles + times\n",
    "        re_string = r'\\b(' + '|'.join(patterns) + r')\\b(\\s|-)\\b([0-9]{3,4})s?\\b(?i)(?!\\sMYA)\\s?(BCE|BC)?'\n",
    "        years = [int(match.group(3)) * (-2*bool(match.group(4))+1)\n",
    "                for match in re.finditer(re_string, text, re.IGNORECASE)]\n",
    "        re_string = r'([0-9]{1,2})(st|nd|rd|th) century\\s?(BCE|BC)?'\n",
    "        centuries = [(int(match.group(1)) * 100 - 100) * (-2*bool(match.group(2))+1)\n",
    "                     for match in re.finditer(re_string, text, re.IGNORECASE)]\n",
    "        years += centuries\n",
    "        years = [y for y in years if y<Dump.MAX_YEAR]\n",
    "        return sorted(years + centuries)\n",
    "    \n",
    "# import sys\n",
    "# !{sys.executable} -m pip install networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_base = '/Users/sppatankar/Developer/My Passport/Curiosity/Data/'\n",
    "name_xml = 'enwiki-20190801-pages-articles-multistream.xml.bz2'\n",
    "name_index = 'enwiki-20190801-pages-articles-multistream-index.txt.bz2'\n",
    "path_xml = path_base + name_xml\n",
    "path_index = path_base + name_index\n",
    "dump = Dump(path_xml, path_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not process Elaz%C4%B1%C4%9F.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Palu, Elaz%C4%B1%C4%9F'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import codecs\n",
    "import copy\n",
    "import re\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "from scipy.io import savemat\n",
    "import networkx as nx\n",
    "\n",
    "def clean_entity_name(name):\n",
    "    \n",
    "    name = name.replace('/wiki/', '') # remove leading address sub-string\n",
    "    name = name.split(\"_\")\n",
    "    processed_string = []\n",
    "    for token in name:\n",
    "        try:\n",
    "            token_copy = copy.deepcopy(token)\n",
    "            # Find and replace six characters long encodings (they are divided in pairs by % signs)\n",
    "            raw_decoding_tags_1 = re.findall('%[a-zA-Z\\d]{2}%[a-zA-Z\\d]{2}%[a-zA-Z\\d]{2}', token_copy)\n",
    "            processed_decoding_tags_1 = {}\n",
    "            for idx, tag in enumerate(raw_decoding_tags_1):\n",
    "                tag = tag.replace('%', '')\n",
    "                tag = tag.lower()\n",
    "                decoding = codecs.decode(tag, \"hex\").decode('utf-8')\n",
    "                processed_decoding_tags_1[raw_decoding_tags_1[idx]] = decoding\n",
    "            for key, value in processed_decoding_tags_1.items():\n",
    "                token_copy = token_copy.replace(key, value) \n",
    "            # Find and replace four characters long encodings\n",
    "            raw_decoding_tags_2 = re.findall('%[a-zA-Z\\d]{2}%[a-zA-Z\\d]{2}', token_copy)\n",
    "            processed_decoding_tags_2 = {}\n",
    "            for idx, tag in enumerate(raw_decoding_tags_2):\n",
    "                tag = tag.replace('%', '')\n",
    "                tag = tag.lower()\n",
    "                decoding = codecs.decode(tag, \"hex\").decode('utf-8')\n",
    "                processed_decoding_tags_2[raw_decoding_tags_2[idx]] = decoding\n",
    "            for key, value in processed_decoding_tags_2.items():\n",
    "                token_copy = token_copy.replace(key, value)\n",
    "            # Find and replace two characters long encodings\n",
    "            raw_decoding_tags_3 = re.findall('%[a-zA-Z\\d]{2}', token_copy)\n",
    "            processed_decoding_tags_3 = {}\n",
    "            for idx, tag in enumerate(raw_decoding_tags_3):\n",
    "                tag = tag.replace('%', '')\n",
    "                tag = tag.lower()\n",
    "                decoding = codecs.decode(tag, \"hex\").decode('utf-8')\n",
    "                processed_decoding_tags_3[raw_decoding_tags_3[idx]] = decoding\n",
    "            for key, value in processed_decoding_tags_3.items():\n",
    "                token_copy = token_copy.replace(key, value)\n",
    "            processed_string.append(token_copy)\n",
    "        except UnicodeDecodeError:\n",
    "            print('Could not process %s.' % token)\n",
    "            processed_string.append(token_copy)\n",
    "\n",
    "    # print(' '.join(processed_string))\n",
    "    return ' '.join(processed_string)\n",
    "\n",
    "test_string = 'Mosque%E2%80%93Cathedral_of_C%C3%B3rdoba'\n",
    "clean_entity_name(test_string)\n",
    "\n",
    "test_string = 'Bah%C3%A1%27%C3%AD_Faith'\n",
    "clean_entity_name(test_string)\n",
    "\n",
    "test_string = '/wiki/99_and_44/100%25_Dead'\n",
    "clean_entity_name(test_string)\n",
    "\n",
    "test_string = 'Camden%2C_New_Jersey'\n",
    "clean_entity_name(test_string)\n",
    "\n",
    "test_string = 'Palu,_Elaz%C4%B1%C4%9F'\n",
    "clean_entity_name(test_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>SourceName</th>\n",
       "      <th>TargetName</th>\n",
       "      <th>Day</th>\n",
       "      <th>TimeOrder</th>\n",
       "      <th>Hyperlink</th>\n",
       "      <th>DistanceWeights</th>\n",
       "      <th>AgeYears</th>\n",
       "      <th>SexOrient</th>\n",
       "      <th>Race</th>\n",
       "      <th>GenderFactor</th>\n",
       "      <th>EducDeg</th>\n",
       "      <th>Income</th>\n",
       "      <th>JE_5D</th>\n",
       "      <th>DS_5D</th>\n",
       "      <th>ST_5D</th>\n",
       "      <th>SC_5D</th>\n",
       "      <th>TS_5D</th>\n",
       "      <th>Count</th>\n",
       "      <th>Weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101</td>\n",
       "      <td>/wiki/Jeff_Bezos</td>\n",
       "      <td>/wiki/Cloud_infrastructure</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.27945</td>\n",
       "      <td>Heterosexual</td>\n",
       "      <td>AsiaAm</td>\n",
       "      <td>0</td>\n",
       "      <td>BachDegree</td>\n",
       "      <td>20to49k</td>\n",
       "      <td>4.4</td>\n",
       "      <td>4.25</td>\n",
       "      <td>1.6</td>\n",
       "      <td>2.8</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101</td>\n",
       "      <td>/wiki/Cloud_infrastructure</td>\n",
       "      <td>/wiki/Cloud_computing_security</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>yes</td>\n",
       "      <td>0.2</td>\n",
       "      <td>23.27945</td>\n",
       "      <td>Heterosexual</td>\n",
       "      <td>AsiaAm</td>\n",
       "      <td>0</td>\n",
       "      <td>BachDegree</td>\n",
       "      <td>20to49k</td>\n",
       "      <td>4.4</td>\n",
       "      <td>4.25</td>\n",
       "      <td>1.6</td>\n",
       "      <td>2.8</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>101</td>\n",
       "      <td>/wiki/Cloud_computing_security</td>\n",
       "      <td>/wiki/Cloud_infrastructure</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>no</td>\n",
       "      <td>0.2</td>\n",
       "      <td>23.27945</td>\n",
       "      <td>Heterosexual</td>\n",
       "      <td>AsiaAm</td>\n",
       "      <td>0</td>\n",
       "      <td>BachDegree</td>\n",
       "      <td>20to49k</td>\n",
       "      <td>4.4</td>\n",
       "      <td>4.25</td>\n",
       "      <td>1.6</td>\n",
       "      <td>2.8</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ID                      SourceName                      TargetName  Day  \\\n",
       "0  101                /wiki/Jeff_Bezos      /wiki/Cloud_infrastructure    1   \n",
       "1  101      /wiki/Cloud_infrastructure  /wiki/Cloud_computing_security    1   \n",
       "2  101  /wiki/Cloud_computing_security      /wiki/Cloud_infrastructure    1   \n",
       "\n",
       "   TimeOrder Hyperlink  DistanceWeights  AgeYears     SexOrient    Race  \\\n",
       "0          1        no              1.0  23.27945  Heterosexual  AsiaAm   \n",
       "1          2       yes              0.2  23.27945  Heterosexual  AsiaAm   \n",
       "2          3        no              0.2  23.27945  Heterosexual  AsiaAm   \n",
       "\n",
       "   GenderFactor     EducDeg   Income  JE_5D  DS_5D  ST_5D  SC_5D  TS_5D  \\\n",
       "0             0  BachDegree  20to49k    4.4   4.25    1.6    2.8    2.0   \n",
       "1             0  BachDegree  20to49k    4.4   4.25    1.6    2.8    2.0   \n",
       "2             0  BachDegree  20to49k    4.4   4.25    1.6    2.8    2.0   \n",
       "\n",
       "   Count  Weight  \n",
       "0      1     0.0  \n",
       "1      2     0.8  \n",
       "2      3     0.8  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_df = pd.read_csv(path_base + 'KNOT_data_raw.csv')\n",
    "wiki_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not process Elaz%C4%B1%C4%9F.\n",
      "Could not process Dav%C3%AD%C3%B0sd%C3%B3ttir.\n",
      "Could not process %C3%9E%C3%B3risd%C3%B3ttir.\n",
      "Could not process %C4%90%C3%ACnh.\n",
      "Could not process Dvo%C5%99%C3%A1k.\n",
      "Could not process Elaz%C4%B1%C4%9F.\n",
      "Could not process Dav%C3%AD%C3%B0sd%C3%B3ttir.\n",
      "Could not process %C3%9E%C3%B3risd%C3%B3ttir.\n",
      "Could not process %C4%90%C3%ACnh.\n",
      "Could not process Dvo%C5%99%C3%A1k.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>SourceName</th>\n",
       "      <th>TargetName</th>\n",
       "      <th>Day</th>\n",
       "      <th>TimeOrder</th>\n",
       "      <th>Hyperlink</th>\n",
       "      <th>DistanceWeights</th>\n",
       "      <th>AgeYears</th>\n",
       "      <th>SexOrient</th>\n",
       "      <th>Race</th>\n",
       "      <th>...</th>\n",
       "      <th>DS_5D</th>\n",
       "      <th>ST_5D</th>\n",
       "      <th>SC_5D</th>\n",
       "      <th>TS_5D</th>\n",
       "      <th>Count</th>\n",
       "      <th>Weight</th>\n",
       "      <th>SourceUID</th>\n",
       "      <th>SrcNameClean</th>\n",
       "      <th>TargetUID</th>\n",
       "      <th>TgtNameClean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101</td>\n",
       "      <td>/wiki/Jeff_Bezos</td>\n",
       "      <td>/wiki/Cloud_infrastructure</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.27945</td>\n",
       "      <td>Heterosexual</td>\n",
       "      <td>AsiaAm</td>\n",
       "      <td>...</td>\n",
       "      <td>4.25</td>\n",
       "      <td>1.6</td>\n",
       "      <td>2.8</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10617</td>\n",
       "      <td>Jeff Bezos</td>\n",
       "      <td>8349</td>\n",
       "      <td>Cloud infrastructure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101</td>\n",
       "      <td>/wiki/Cloud_infrastructure</td>\n",
       "      <td>/wiki/Cloud_computing_security</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>yes</td>\n",
       "      <td>0.2</td>\n",
       "      <td>23.27945</td>\n",
       "      <td>Heterosexual</td>\n",
       "      <td>AsiaAm</td>\n",
       "      <td>...</td>\n",
       "      <td>4.25</td>\n",
       "      <td>1.6</td>\n",
       "      <td>2.8</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.8</td>\n",
       "      <td>8349</td>\n",
       "      <td>Cloud infrastructure</td>\n",
       "      <td>412</td>\n",
       "      <td>Cloud computing security</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>101</td>\n",
       "      <td>/wiki/Cloud_computing_security</td>\n",
       "      <td>/wiki/Cloud_infrastructure</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>no</td>\n",
       "      <td>0.2</td>\n",
       "      <td>23.27945</td>\n",
       "      <td>Heterosexual</td>\n",
       "      <td>AsiaAm</td>\n",
       "      <td>...</td>\n",
       "      <td>4.25</td>\n",
       "      <td>1.6</td>\n",
       "      <td>2.8</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.8</td>\n",
       "      <td>412</td>\n",
       "      <td>Cloud computing security</td>\n",
       "      <td>8349</td>\n",
       "      <td>Cloud infrastructure</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    ID                      SourceName                      TargetName  Day  \\\n",
       "0  101                /wiki/Jeff_Bezos      /wiki/Cloud_infrastructure    1   \n",
       "1  101      /wiki/Cloud_infrastructure  /wiki/Cloud_computing_security    1   \n",
       "2  101  /wiki/Cloud_computing_security      /wiki/Cloud_infrastructure    1   \n",
       "\n",
       "   TimeOrder Hyperlink  DistanceWeights  AgeYears     SexOrient    Race  ...  \\\n",
       "0          1        no              1.0  23.27945  Heterosexual  AsiaAm  ...   \n",
       "1          2       yes              0.2  23.27945  Heterosexual  AsiaAm  ...   \n",
       "2          3        no              0.2  23.27945  Heterosexual  AsiaAm  ...   \n",
       "\n",
       "   DS_5D ST_5D SC_5D  TS_5D  Count  Weight  SourceUID  \\\n",
       "0   4.25   1.6   2.8    2.0      1     0.0      10617   \n",
       "1   4.25   1.6   2.8    2.0      2     0.8       8349   \n",
       "2   4.25   1.6   2.8    2.0      3     0.8        412   \n",
       "\n",
       "               SrcNameClean  TargetUID              TgtNameClean  \n",
       "0                Jeff Bezos       8349      Cloud infrastructure  \n",
       "1      Cloud infrastructure        412  Cloud computing security  \n",
       "2  Cloud computing security       8349      Cloud infrastructure  \n",
       "\n",
       "[3 rows x 24 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create UID for each page\n",
    "source_nodes = set(wiki_df['SourceName'].tolist())\n",
    "target_nodes = set(wiki_df['TargetName'].tolist())\n",
    "source_nodes.update(target_nodes)\n",
    "node_set = {entity: name for name, entity in enumerate(source_nodes)}\n",
    "wiki_df['SourceUID'] = wiki_df['SourceName'].apply(lambda x: node_set[x])\n",
    "wiki_df['SrcNameClean'] = wiki_df['SourceName'].apply(lambda x: clean_entity_name(x))\n",
    "wiki_df['TargetUID'] = wiki_df['TargetName'].apply(lambda x: node_set[x])\n",
    "wiki_df['TgtNameClean'] = wiki_df['TargetName'].apply(lambda x: clean_entity_name(x))\n",
    "wiki_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sppatankar/opt/anaconda3/envs/everything/lib/python3.7/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network built for subject 101\n",
      "311 309\n",
      "Network built for subject 104\n",
      "148 148\n",
      "Network built for subject 105\n",
      "100 100\n",
      "Network built for subject 106\n",
      "476 476\n",
      "Network built for subject 107\n",
      "202 198\n",
      "Network built for subject 108\n",
      "134 134\n",
      "Network built for subject 109\n",
      "135 135\n",
      "Network built for subject 112\n",
      "89 89\n",
      "Network built for subject 114\n",
      "127 127\n",
      "Network built for subject 115\n",
      "398 393\n",
      "Network built for subject 117\n",
      "165 165\n",
      "Network built for subject 119\n",
      "255 255\n",
      "Network built for subject 120\n",
      "111 111\n",
      "Network built for subject 121\n",
      "321 315\n",
      "Network built for subject 122\n",
      "346 344\n",
      "Network built for subject 126\n",
      "246 246\n",
      "Network built for subject 127\n",
      "179 179\n",
      "Network built for subject 128\n",
      "151 151\n",
      "Network built for subject 130\n",
      "337 335\n",
      "Network built for subject 131\n",
      "287 287\n",
      "Network built for subject 132\n",
      "124 124\n",
      "Network built for subject 135\n",
      "215 211\n",
      "Network built for subject 138\n",
      "199 195\n",
      "Network built for subject 139\n",
      "261 259\n",
      "Network built for subject 140\n",
      "185 185\n",
      "Network built for subject 141\n",
      "560 556\n",
      "Network built for subject 146\n",
      "132 130\n",
      "Network built for subject 150\n",
      "58 52\n",
      "Network built for subject 153\n",
      "418 398\n",
      "Network built for subject 154\n",
      "209 207\n",
      "Network built for subject 155\n",
      "242 240\n",
      "Network built for subject 156\n",
      "100 100\n",
      "Network built for subject 157\n",
      "213 213\n",
      "Network built for subject 158\n",
      "114 112\n",
      "Network built for subject 159\n",
      "261 257\n",
      "Network built for subject 162\n",
      "152 144\n",
      "Network built for subject 164\n",
      "261 261\n",
      "Network built for subject 165\n",
      "57 57\n",
      "Network built for subject 167\n",
      "88 84\n",
      "Network built for subject 169\n",
      "313 311\n",
      "Network built for subject 171\n",
      "37 37\n",
      "Network built for subject 173\n",
      "60 60\n",
      "Network built for subject 174\n",
      "109 109\n",
      "Network built for subject 176\n",
      "140 140\n",
      "Network built for subject 177\n",
      "118 108\n",
      "Network built for subject 179\n",
      "64 64\n",
      "Network built for subject 183\n",
      "285 285\n",
      "Network built for subject 185\n",
      "330 330\n",
      "Network built for subject 188\n",
      "272 268\n",
      "Network built for subject 189\n",
      "97 95\n",
      "Network built for subject 190\n",
      "145 143\n",
      "Network built for subject 191\n",
      "117 117\n",
      "Network built for subject 192\n",
      "164 162\n",
      "Network built for subject 194\n",
      "62 60\n",
      "Network built for subject 196\n",
      "195 195\n",
      "Network built for subject 197\n",
      "124 122\n",
      "Network built for subject 198\n",
      "391 387\n",
      "Network built for subject 199\n",
      "392 388\n",
      "Network built for subject 201\n",
      "89 89\n",
      "Network built for subject 204\n",
      "71 71\n",
      "Network built for subject 206\n",
      "39 39\n",
      "Network built for subject 207\n",
      "189 187\n",
      "Network built for subject 208\n",
      "118 116\n",
      "Network built for subject 209\n",
      "84 80\n",
      "Network built for subject 210\n",
      "147 147\n",
      "Network built for subject 211\n",
      "159 159\n",
      "Network built for subject 212\n",
      "213 213\n",
      "Network built for subject 214\n",
      "154 154\n",
      "Network built for subject 216\n",
      "322 320\n",
      "Network built for subject 217\n",
      "262 260\n",
      "Network built for subject 219\n",
      "225 225\n",
      "Network built for subject 220\n",
      "293 287\n",
      "Network built for subject 221\n",
      "107 105\n",
      "Network built for subject 223\n",
      "104 102\n",
      "Network built for subject 224\n",
      "225 219\n",
      "Network built for subject 225\n",
      "113 113\n",
      "Network built for subject 226\n",
      "261 256\n",
      "Network built for subject 228\n",
      "112 112\n",
      "Network built for subject 229\n",
      "426 426\n",
      "Network built for subject 231\n",
      "255 253\n",
      "Network built for subject 232\n",
      "215 213\n",
      "Network built for subject 234\n",
      "271 265\n",
      "Network built for subject 235\n",
      "415 415\n",
      "Network built for subject 236\n",
      "129 127\n",
      "Network built for subject 238\n",
      "208 206\n",
      "Network built for subject 239\n",
      "183 173\n",
      "Network built for subject 240\n",
      "362 356\n",
      "Network built for subject 242\n",
      "179 179\n",
      "Network built for subject 243\n",
      "110 110\n",
      "Network built for subject 246\n",
      "268 264\n",
      "Network built for subject 247\n",
      "479 469\n",
      "Network built for subject 248\n",
      "131 129\n",
      "Network built for subject 249\n",
      "142 142\n",
      "Network built for subject 251\n",
      "102 102\n",
      "Network built for subject 253\n",
      "58 58\n",
      "Network built for subject 255\n",
      "65 65\n",
      "Network built for subject 256\n",
      "100 98\n",
      "Network built for subject 258\n",
      "237 235\n",
      "Network built for subject 259\n",
      "194 194\n",
      "Network built for subject 261\n",
      "292 288\n",
      "Network built for subject 263\n",
      "28 28\n",
      "Network built for subject 264\n",
      "163 159\n",
      "Network built for subject 266\n",
      "172 170\n",
      "Network built for subject 267\n",
      "110 106\n",
      "Network built for subject 268\n",
      "90 90\n",
      "Network built for subject 269\n",
      "168 158\n",
      "Network built for subject 271\n",
      "76 76\n",
      "Network built for subject 273\n",
      "160 160\n",
      "Network built for subject 278\n",
      "161 161\n",
      "Network built for subject 280\n",
      "131 131\n",
      "Network built for subject 286\n",
      "72 72\n",
      "Network built for subject 287\n",
      "94 94\n",
      "Network built for subject 288\n",
      "221 219\n",
      "Network built for subject 290\n",
      "191 191\n",
      "Network built for subject 291\n",
      "127 127\n",
      "Network built for subject 293\n",
      "254 254\n",
      "Network built for subject 296\n",
      "240 231\n",
      "Network built for subject 297\n",
      "301 301\n",
      "Network built for subject 304\n",
      "210 210\n",
      "Network built for subject 308\n",
      "114 114\n",
      "Network built for subject 309\n",
      "91 91\n",
      "Network built for subject 310\n",
      "169 169\n",
      "Network built for subject 311\n",
      "160 160\n",
      "Network built for subject 312\n",
      "73 73\n",
      "Network built for subject 313\n",
      "110 108\n",
      "Network built for subject 316\n",
      "233 231\n",
      "Network built for subject 318\n",
      "205 203\n",
      "Network built for subject 319\n",
      "135 133\n",
      "Network built for subject 321\n",
      "185 185\n",
      "Network built for subject 322\n",
      "81 81\n",
      "Network built for subject 323\n",
      "151 149\n",
      "Network built for subject 324\n",
      "134 132\n",
      "Network built for subject 325\n",
      "96 96\n",
      "Network built for subject 327\n",
      "263 261\n",
      "Network built for subject 328\n",
      "185 185\n",
      "Network built for subject 329\n",
      "278 278\n",
      "Network built for subject 335\n",
      "182 182\n",
      "Network built for subject 338\n",
      "318 318\n",
      "Network built for subject 339\n",
      "59 59\n",
      "Network built for subject 340\n",
      "214 214\n",
      "Network built for subject 342\n",
      "102 102\n",
      "Network built for subject 349\n",
      "276 274\n",
      "Network built for subject 351\n",
      "221 217\n",
      "Network built for subject 353\n",
      "175 175\n",
      "Network built for subject 355\n",
      "95 95\n",
      "Network built for subject 356\n",
      "227 227\n",
      "Network built for subject 359\n",
      "112 112\n",
      "Network built for subject 363\n",
      "257 257\n",
      "Network built for subject 340340\n",
      "209 209\n"
     ]
    }
   ],
   "source": [
    "# split the data by individual\n",
    "ID_groups = wiki_df.groupby('ID')\n",
    "for ID, group in ID_groups:\n",
    "    count = 0\n",
    "    # enforce time ordering\n",
    "    group.sort_values(by = ['TimeOrder'], inplace = True)\n",
    "    network_df = group[['TimeOrder', 'SourceUID', 'SrcNameClean', 'TargetUID', 'TgtNameClean']].reset_index(drop = True)\n",
    "    G = nx.Graph()\n",
    "    links = {}\n",
    "    for index, row in network_df.iterrows():\n",
    "        from_node = row.get('SrcNameClean')\n",
    "        to_node = row.get('TgtNameClean')\n",
    "        tag = 0\n",
    "        if from_node in dump.idx.keys():\n",
    "            if to_node in dump.idx.keys():\n",
    "                tag = 1\n",
    "                count += 1\n",
    "        # print(from_node, ', ',to_node, tag)\n",
    "        # add nodes to the network\n",
    "        G.add_node(from_node)\n",
    "        G.add_node(to_node)\n",
    "    print(\"Network built for subject %d\" % ID)\n",
    "    print(len(network_df), count)\n",
    "        \n",
    "    \n",
    "#     # create an empty network\n",
    "#     G = nx.Graph()\n",
    "#     edge_info = []\n",
    "#     for index, row in network_df.iterrows():\n",
    "#         from_node = row.get('SrcNameClean')\n",
    "#         to_node = row.get('TgtNameClean')\n",
    "#         edge_info_dict = {'from': from_node, 'to': to_node}\n",
    "#         edge_info.append(edge_info_dict)\n",
    "#         # add nodes to the network\n",
    "#         G.add_node(from_node)\n",
    "#         G.add_node(to_node)\n",
    "#         # add edge to the network\n",
    "#         G.add_edge(from_node, to_node)\n",
    "#     adj_G = nx.linalg.graphmatrix.adjacency_matrix(G, weight = 'weight')\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = ['optics']\n",
    "links = {}\n",
    "for topic in topics:\n",
    "    dump.load_page('Index of %s articles' % topic)\n",
    "    links[topic] = [str(l) for l in dump.article_links]\n",
    "    print('Topic \"' + topic + '\" has ' + str(len(links[topic])) + ' articles.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump.idx.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get index of articles\n",
    "\n",
    "* [all indices on Wikipedia](https://en.wikipedia.org/wiki/Portal:Contents/Indices)\n",
    "* topics not searched\n",
    "* international trade (\"topics\"), theory of constraints (small)\n",
    "* too big: mathematics, neuroscience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path_base = '/Users/harangju/Developer/data/wiki/dumps/'\n",
    "name_xml = 'enwiki-20190801-pages-articles-multistream.xml.bz2'\n",
    "name_index = 'enwiki-20190801-pages-articles-multistream-index.txt.bz2'\n",
    "path_xml = path_base + name_xml\n",
    "path_index = path_base + name_index\n",
    "dump = wiki.Dump(path_xml, path_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# natural & physical sciences\n",
    "topics = ['anatomy', 'biochemistry', 'cognitive science', 'evolutionary biology',\n",
    "          'genetics', 'immunology', 'molecular biology']\n",
    "topics += ['chemistry', 'biophysics', 'energy', 'optics', \n",
    "           'earth science', 'geology', 'meteorology']\n",
    "# philosophy\n",
    "# topics += []\n",
    "topics += ['philosophy of language', 'philosophy of law', \n",
    "           'philosophy of mind', 'philosophy of science']\n",
    "# social sciences\n",
    "topics += ['economics', 'accounting', 'education', 'linguistics', 'law', 'psychology',\n",
    "           'sociology']\n",
    "# technology & applied sciences\n",
    "topics += ['electronics', 'software engineering', 'robotics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "links = {}\n",
    "for topic in topics:\n",
    "    dump.load_page('Index of %s articles' % topic)\n",
    "    links[topic] = [str(l) for l in dump.article_links]\n",
    "    print('Topic \"' + topic + '\" has ' + str(len(links[topic])) + ' articles.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# https://en.wikipedia.org/wiki/Lists_of_mathematics_topics\n",
    "# algebra\n",
    "math_topics = ['calculus', 'geometry', 'abstract algebra',\n",
    "               'Boolean algebra', 'commutative algebra',# 'homological algebra',\n",
    "               'group theory',# 'representation theory', \n",
    "               'linear algebra']\n",
    "# calculus & analysis\n",
    "# math_topics += ['complex analysis', 'functional analysis',\n",
    "#                 'integration and measure theory', 'harmonic analysis',\n",
    "#                 'Fourier analysis', 'multivariable calculus', 'real analysis',\n",
    "#                 'variational']\n",
    "# geometry\n",
    "# math_topics += ['geometry', 'curves', 'triangle', 'circle', 'general topology',\n",
    "#                 'differential geometry', 'algebraic geometry', 'algebraic topology',\n",
    "#                 'geometric topology', 'know theory', 'Lie groups']\n",
    "# number theory\n",
    "math_topics += [#'algebraic number theory',\n",
    "                'number theory']\n",
    "# applied math\n",
    "math_topics += ['dynamical systems and differential equations']\n",
    "#                 'partial differential equation']\n",
    "topics += math_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "links = {}\n",
    "for topic in math_topics:\n",
    "    dump.load_page(f\"List of {topic} topics\")\n",
    "    links[topic] = [str(l) for l in dump.article_links]\n",
    "    print('Topic \"' + topic + '\" has ' + str(len(links[topic])) + ' articles.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "topics += ['physics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "topic = 'physics'\n",
    "links[topic] = []\n",
    "for letter in ['!$@', '0–9'] + list(string.ascii_uppercase):\n",
    "    dump.load_page('Index of physics articles (%s)' % letter)\n",
    "    links[topic].extend([str(l) for l in dump.article_links])\n",
    "print('Topic \"' + topic + '\" has ' + str(len(links[topic])) + ' articles.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "topics += ['mathematics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "topic = 'mathematics'\n",
    "links[topic] = []\n",
    "for letter in ['0–9'] + list(string.ascii_uppercase):\n",
    "    dump.load_page(\n",
    "        f\"Wikipedia:WikiProject Mathematics/List of mathematics articles ({letter})'\n",
    "    )\n",
    "    links[topic].extend([str(l) for l in dump.article_links])\n",
    "print('Topic \"' + topic + '\" has ' + str(len(links[topic])) + ' articles.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build graphs of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gensim.utils as gu\n",
    "\n",
    "path_models = '/Users/harangju/Developer/data/wiki/models/'\n",
    "tfidf = gu.SaveLoad.load(path_models + 'tfidf.model')\n",
    "dct = pickle.load(open(path_models + 'dict.model','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One network per topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "path = '/Users/harangju/Developer/data/wiki/graphs/dated/'\n",
    "\n",
    "networks = {}\n",
    "for topic in topics:\n",
    "    ls = links[topic]\n",
    "    print('\\nTopic: ' + topic)\n",
    "    networks[topic] = wiki.Net()\n",
    "    networks[topic].build_graph(\n",
    "        name=topic, dump=dump, nodes=ls, model=tfidf, dct=dct\n",
    "    )\n",
    "    networks[topic].save_graph(path + topic + '.pickle')\n",
    "    networks[topic].save_graph(path + topic + '.gexf')\n",
    "    networks[topic].save_barcodes(path + topic + '.barcode')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redo barcodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path_saved = '/Users/harangju/Developer/data/wiki/graphs/dated/'\n",
    "networks = {}\n",
    "for topic in topics:\n",
    "    print(topic, end=' ')\n",
    "    networks[topic] = wiki.Net(\n",
    "        path_graph=os.path.join(path_saved, topic+'.pickle'),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = '/Users/harangju/Developer/data/wiki/graphs/barcode/'\n",
    "\n",
    "for topic in topics:\n",
    "    print('\\nTopic: ' + topic)\n",
    "    networks[topic].save_barcodes(os.path.join(path, topic+'.barcode'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subnetworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_links = list(set([v for l in links.values() for v in l]))\n",
    "len(all_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = '/Users/harangju/Developer/data/wiki/graphs/dated/'\n",
    "\n",
    "big_network = wiki.Net()\n",
    "big_network.build_graph(\n",
    "    name='big_network',\n",
    "    dump=dump, \n",
    "    nodes=all_links, \n",
    "    model=tfidf, \n",
    "    dct=dct,\n",
    "    compute_core_periphery=False, \n",
    "    compute_communities=False, \n",
    "    compute_community_cores=False\n",
    ")\n",
    "big_network.save_graph(os.path.join(path, 'big_network_physics_math.pickle'))\n",
    "big_network.save_graph(os.path.join(path, 'big_network_physics_math.gexf'))\n",
    "# big_network.save_barcodes(os.path.join(path, 'big_network.barcode'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nodes without years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "path = '/Users/harangju/Developer/data/wiki/graphs/dated-noyear/'\n",
    "\n",
    "if not os.path.isdir(path):\n",
    "    os.mkdir(path)\n",
    "\n",
    "networks_noyear = {}\n",
    "for topic in topics:\n",
    "    print('\\nTopic: ' + topic)\n",
    "    networks_noyear[topic] = wiki.Net()\n",
    "    networks_noyear[topic].build_graph(\n",
    "        name=topic, dump=dump, nodes=links[topic],\n",
    "        fill_empty_years=False,\n",
    "        compute_core_periphery=False,\n",
    "        compute_communities=False,\n",
    "        compute_community_cores=False\n",
    "    )\n",
    "    networks_noyear[topic].save_graph(path + topic + '.pickle')\n",
    "    networks_noyear[topic].save_graph(path + topic + '.gexf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "path = '/Users/harangju/Developer/data/wiki/graphs/dated-noyear/'\n",
    "\n",
    "networks_noyear = {}\n",
    "for topic in topics:\n",
    "    print(topic, end=' ')\n",
    "    networks_noyear[topic] = wiki.Net()\n",
    "    networks_noyear[topic].load_graph(path + topic + '.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import plotly.express as px\n",
    "\n",
    "fraction_years = pd.DataFrame(\n",
    "    [\n",
    "        [\n",
    "            topic,\n",
    "            len([\n",
    "                y\n",
    "                for n, y in nx.get_node_attributes(\n",
    "                    networks_noyear[topic].graph, 'year'\n",
    "                ).items()\n",
    "                if y\n",
    "            ]) / len(networks_noyear[topic].graph.nodes)\n",
    "        ]\n",
    "        for topic in topics\n",
    "    ],\n",
    "    columns=['topics', 'fraction']\n",
    ")\n",
    "fraction_years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for topic in topics:\n",
    "#     print(topic, end='\\t')\n",
    "#     print( \n",
    "#         len([\n",
    "#             y\n",
    "#             for n, y in nx.get_node_attributes(\n",
    "#                 networks_noyear[topic].graph, 'year'\n",
    "#             ).items()\n",
    "#             if y\n",
    "#         ]) / len(networks_noyear[topic].graph.nodes)\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path_fig = '/Users/harangju/Library/Mobile Documents/com~apple~CloudDocs/' +\\\n",
    "    'Documents/research/wikipedia/results'\n",
    "path_plot = '0 graphs'\n",
    "\n",
    "fig = px.histogram(fraction_years.fraction)\n",
    "fig.update_layout(\n",
    "    width=500, height=360,\n",
    "    template='plotly_white',\n",
    "    xaxis={'range': [0, 1]},\n",
    "    showlegend=False\n",
    ")\n",
    "fig.show()\n",
    "fig.write_image(os.path.join(path_fig, path_plot, 'fraction_years_with_math.pdf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate null networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path_saved = '/Users/harangju/Developer/data/wiki/graphs/dated/'\n",
    "networks = {}\n",
    "for topic in topics:\n",
    "    print(topic, end=' ')\n",
    "    networks[topic] = wiki.Net()\n",
    "    networks[topic].load_graph(path_saved + topic + '.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomized target & year\n",
    "\n",
    "Just randomizing year -> you get the same structures, it's just a matter of when you get those structures.\n",
    "If you randomize year & target, then you're randomizing the structure & how they come about without changing any basic network statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "null_models = ['target', 'year']\n",
    "num_nulls = 10\n",
    "nulls = {}\n",
    "for null_model in null_models:\n",
    "    print('Null model: ' + null_model)\n",
    "    path_to_save_null = '/Users/harangju/Developer/data/wiki/graphs/null-'\\\n",
    "                        +null_model+'/'\n",
    "    nulls[null_model] = {}\n",
    "    for topic, network in networks.items():\n",
    "        print('Topic: ' + topic)\n",
    "        nulls[null_model][topic] = []\n",
    "        for i in range(num_nulls):\n",
    "            print('Null: ' + str(i))\n",
    "            null = network.randomize(null_model)\n",
    "            null.graph.name = topic+'-null-'+str(i)\n",
    "            null.save_graph(path_to_save_null + null.graph.name + '.pickle')\n",
    "            null.save_barcodes(path_to_save_null + null.graph.name + '.barcode')\n",
    "            nulls[null_model][topic].append(null)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jittered years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_jitters = 1\n",
    "max_jitter = 1\n",
    "null_model = 'jitter'\n",
    "path_to_save_null = '/Users/harangju/Developer/data/wiki/graphs/null-'+null_model+'/'\n",
    "if not os.path.isdir(path_to_save_null):\n",
    "    os.mkdir(path_to_save_null)\n",
    "jittered = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import numpy.random\n",
    "\n",
    "for topic, network in networks.items():\n",
    "    print('Topic: ' + topic)\n",
    "    jittered[topic] = []\n",
    "    print('Null: ', end='')\n",
    "    for i in range(num_jitters):\n",
    "        print(str(i), end=' ')\n",
    "        null = wiki.Net()\n",
    "        null.graph = copy.deepcopy(network.graph)\n",
    "        for node in null.graph.nodes:\n",
    "            null.graph.nodes[node]['year'] = null.graph.nodes[node]['year'] +\\\n",
    "                np.random.randint(-max_jitter, max_jitter+1)\n",
    "        null.graph.name = topic+'-null-'+str(i)\n",
    "        null.save_graph(path_to_save_null + null.graph.name + '.pickle')\n",
    "        null.save_barcodes(path_to_save_null + null.graph.name + '.barcode')\n",
    "        jittered[topic].append(null)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gephi notes\n",
    "* node size, fruchterman reingold = [10, 40], force atlas 2 = [4 16]\n",
    "* text size = [1 1.4]\n",
    "* preview font size = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative networks\n",
    "* random geometric graph (modularity)\n",
    "* stochastic block model (modularity)\n",
    "* caveman graph (modularity, cliques, most clustered & sparse)\n",
    "* random clustered graph (clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_nulls = 10\n",
    "gen_functions = {\n",
    "    'rgg': lambda g: nx.random_geometric_graph(\n",
    "        g.number_of_nodes(), \n",
    "    ),\n",
    "    'sbm': lambda g: nx.stochastic_block_model(\n",
    "        \n",
    "    ),\n",
    "    'cg': lambda g: nx.caveman_graph(\n",
    "        \n",
    "    ),\n",
    "    'rcg': lambda g: nx.random_clustered_graph(\n",
    "        \n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_nulls = 10\n",
    "gen_nulls = {}\n",
    "for name, function in gen_functions.items():\n",
    "    print('Null model: ' + null_model)\n",
    "    path_to_save_null = '/Users/harangju/Developer/data/wiki/graphs/null-'+\\\n",
    "        null_model+'/'\n",
    "    nulls[null_model] = {}\n",
    "    for topic, network in networks.items():\n",
    "        print('Topic: ' + topic)\n",
    "        nulls[null_model][topic] = []\n",
    "        for i in range(num_nulls):\n",
    "            print('Null: ' + str(i))\n",
    "            null = network.randomize(null_model)\n",
    "            null.graph.name = topic+'-null-'+str(i)\n",
    "            null.save_graph(path_to_save_null + null.graph.name + '.pickle')\n",
    "            null.save_barcodes(path_to_save_null + null.graph.name + '.barcode')\n",
    "            nulls[null_model][topic].append(null)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate networks for D3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path_saved = '/Users/harangju/Developer/data/wiki/graphs/dated/'\n",
    "networks = {}\n",
    "for topic in topics:\n",
    "    print(topic, end=' ')\n",
    "    networks[topic] = wiki.Net()\n",
    "    networks[topic].load_graph(path_saved + topic + '.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[(topic, len(networks[topic].graph.nodes)) for topic in topics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json as js\n",
    "import networkx as nx\n",
    "\n",
    "path = '/Users/harangju/Developer/data/wiki/graphs/json/'\n",
    "\n",
    "for topic, network in networks.items():\n",
    "    nodes = sorted(network.graph.nodes, key=lambda n: network.graph.nodes[n]['year'])\n",
    "    json = js.dumps({\n",
    "        'nodes': [\n",
    "            {\n",
    "                'id': node,\n",
    "                'year': int(network.graph.nodes[node]['year']),\n",
    "#                 'core_be': int(network.graph.nodes[node]['core_be']),\n",
    "#                 'core_rb': network.graph.nodes[node]['core_rb'],\n",
    "                'community': int(network.graph.nodes[node]['community']),\n",
    "                'degree': network.graph.degree(node)\n",
    "            }\n",
    "            for i, node in enumerate(nodes)\n",
    "        ],\n",
    "        'links': [\n",
    "            {\n",
    "                'source': i,\n",
    "                'target': nodes.index(target),\n",
    "                'weight': network.graph.edges[node, target]['weight']\n",
    "            }\n",
    "            for i, node in enumerate(nodes)\n",
    "            for target in network.graph.successors(node)\n",
    "        ]\n",
    "    })\n",
    "    with open(os.path.join(path, topic+'.json'), 'w') as file:\n",
    "        file.write(json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate barcodes for D3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path_saved = '/Users/harangju/Developer/data/wiki/graphs/dated/'\n",
    "networks = {}\n",
    "for topic in topics:\n",
    "    print(topic, end=' ')\n",
    "    networks[topic] = wiki.Net(\n",
    "        path_graph=os.path.join(path_saved, topic+'.pickle'),\n",
    "#         path_barcodes=os.path.join(path_saved, topic+'.barcode')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f = networks['cognitive science'].filtration\n",
    "m = networks['cognitive science'].persistence\n",
    "for i, c in enumerate(m):\n",
    "    if m.pair(i) < i: continue      # skip negative simplices\n",
    "    dim = f[i].dimension()\n",
    "    if m.pair(i) != m.unpaired:\n",
    "        print(f\"{i}, {dim}, {c}, {m[i]}, {m.pair(i)}, {m[m.pair(i)]}\")\n",
    "    else:\n",
    "        print(f\"{i}, {dim}, {c}, {m[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m[m.pair(13)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import dionysus as d\n",
    "\n",
    "dgms = d.init_diagrams(\n",
    "    networks['earth science'].persistence,\n",
    "    networks['earth science'].filtration\n",
    ")\n",
    "dgms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, dgm in enumerate(dgms):\n",
    "    print(f\"dim {i}\", end=' ')\n",
    "    for p in dgm:\n",
    "        print(p, end='; ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dgms[2][0], dgms[2][0].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "barcodes = networks['cognitive science'].barcodes.copy()\n",
    "barcodes = barcodes\\\n",
    "    .drop(index=barcodes[barcodes.lifetime==0].index)\\\n",
    "    .reset_index(drop=True)\n",
    "barcodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "barcodes.iloc[27]['death simplex'], barcodes.iloc[27]['homology nodes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = '/Users/harangju/Developer/data/wiki/graphs/barcode csv/'\n",
    "\n",
    "for topic, network in {'cognitive science': networks['cognitive science']}.items(): #networks.items():\n",
    "    barcodes = network.barcodes.copy(deep=True)\n",
    "    barcodes = barcodes\\\n",
    "        .drop(index=barcodes[barcodes.lifetime==0].index)\\\n",
    "        .reset_index(drop=True)\n",
    "    barcodes.death = barcodes.death\\\n",
    "        .replace(np.inf, 2100)\\\n",
    "        .astype(int)\n",
    "    csv = 'i,birth,death,dim,cavity,death_nodes\\n'\n",
    "    for i, row in barcodes.iterrows():\n",
    "        if row.lifetime==np.inf:\n",
    "            cavity = row['birth simplex']\n",
    "        else:\n",
    "            cavity = row['homology nodes']\n",
    "        csv += f\"{i},{row.birth},{row.death},{row.dim},{';'.join(cavity)},\" + \\\n",
    "            f\"{';'.join(row['death nodes'])}\\n\"\n",
    "    with open(os.path.join(path, topic+'.csv'), 'w') as file:\n",
    "        file.write(csv)\n",
    "# print(csv)"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "239.1875px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
